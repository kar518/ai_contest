import torch
from torch import nn, optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import torch.nn.functional as F

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# DataLoader
transform = transforms.ToTensor()
train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
train_loader = DataLoader(train_data, batch_size=128, shuffle=True)

# Simple Generator
class Generator(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(10 + 100, 256),
            nn.ReLU(),
            nn.Linear(256, 28*28),
            nn.Tanh()
        )

    def forward(self, noise, labels):
        x = torch.cat([noise, labels], dim=1)
        out = self.fc(x)
        return out.view(-1, 1, 28, 28)

# Instantiate
generator = Generator().to(device)

# Loss & Optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(generator.parameters(), lr=0.0002)

# Training loop (example: 50 epochs)
for epoch in range(50):
    for images, labels in train_loader:
        batch_size = images.size(0)
        images = images.to(device)

        # One-hot labels
        labels_onehot = F.one_hot(labels, num_classes=10).float().to(device)

        # Random noise
        noise = torch.randn(batch_size, 100).to(device)

        # Fake images
        fake_images = generator(noise, labels_onehot)

        # Loss
        loss = criterion(fake_images, images)

        # Backprop
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f"Epoch {epoch+1}/50, Loss: {loss.item()}")

# Save generator
torch.save(generator.state_dict(), 'generator_mnist.pth')
